{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands on session: Variational Autoencoders (VAE)\n",
    "In this exercise, we reuse the data from the second Autoencoder example and train a Variational Autoencoder (VAE) to reduce the dimensionality of the calcium imaging data. Recap: The dataset contains the spike times points of >100 neurons along with the behavioral data of a mouse.\n",
    "\n",
    "This exercise refers to [Chapter 9 \"Variational Autoencoders (VAE)\"](https://www.fabriziomusacchio.com/teaching/teaching_dimensionality_reduction_in_neuroscience/09_vae) of the \"[Dimensionality reduction in neuroscience](https://www.fabriziomusacchio.com/teaching/teaching_dimensionality_reduction_in_neuroscience/)\" course (tutor: Fabrizio Musacchio, Oct 17, 2024)\n",
    "\n",
    "## Acknowledgements\n",
    "The dataset used here is extracted from the datasets available in the [CEBRA package](https://cebra.ai/docs/demo_notebooks/Demo_hippocampus.html).\n",
    "\n",
    "## Dataset\n",
    "The dataset consists of several sub-structures:\n",
    "- `neuron_spike_times`: A binary matrix of shape `(N_rec, n_timepoints)` where `N_rec=120` is the number of\n",
    "    recorded neurons and `n_timepoints=1000` is the number of timepoints in the recording. A value of 1 indicates\n",
    "    a spike from a neuron at a given timepoint.\n",
    "- `position_readout`: A vector of shape `(n_timepoints)`, representing the position of the mouse at each timepoint.\n",
    "- `left_direction`: A binary vector of shape `(n_timepoints)`, indicating whether the mouse is moving in the left direction.\n",
    "- `right_direction`: A binary vector of shape `(n_timepoints)`, indicating whether the mouse is moving in the right direction.\n",
    "- `N_rec`: The number of recorded neurons.\n",
    "\n",
    "## Environment setup\n",
    "For reproducibility:\n",
    "\n",
    "```bash\n",
    "conda create -n dimredcution python=3.11 mamba -y\n",
    "conda activate dimredcution\n",
    "mamba install ipykernel matplotlib numpy scipy scikit-learn -y\n",
    "mamba install pytorch torchvision -c pytorch\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by loading the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from mpl_toolkits import mplot3d\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# verify torch version and GPU availability:\n",
    "print(f\"torch backend MPS is available? {torch.backends.mps.is_available()}\")\n",
    "print(f\"current PyTorch installation built with MPS activated? {torch.backends.mps.is_built()}\")\n",
    "print(f\"check the torch MPS backend: {torch.device('mps')}\")\n",
    "print(f\"test torch tensor on MPS: {torch.tensor([1,2,3], device='mps')}\")\n",
    "\"\"\" \n",
    "On macOS: Don't forget to move your model to the MPS device, if you want to use it:\n",
    "\n",
    "device = torch.device('mps')\n",
    "model = model.to(device)\n",
    "\"\"\"\n",
    "\n",
    "# set global properties for all plots:\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.rcParams[\"axes.spines.top\"]    = False\n",
    "plt.rcParams[\"axes.spines.bottom\"] = False\n",
    "plt.rcParams[\"axes.spines.left\"]   = False\n",
    "plt.rcParams[\"axes.spines.right\"]  = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the path to the data file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/'\n",
    "DATA_FILENAME = 'hippocampus_achilles_from_cebra_dict.pkl'\n",
    "DATA_FILE = os.path.join(DATA_PATH, DATA_FILENAME)\n",
    "\n",
    "RESULTSPATH = '../results/'\n",
    "# check whether the results path exists, if not, create it:\n",
    "if not os.path.exists(RESULTSPATH):\n",
    "    os.makedirs(RESULTSPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% LOAD THE DATA\n",
    "hippocampus_achilles_dict = pickle.load(open(DATA_FILE, 'rb'))\n",
    "\n",
    "neuron_spike_times = hippocampus_achilles_dict['neuron_spike_times']\n",
    "position_readout   = hippocampus_achilles_dict['position_readout']\n",
    "left_direction     = hippocampus_achilles_dict['left_direction']\n",
    "right_direction    = hippocampus_achilles_dict['right_direction']\n",
    "N_rec              = hippocampus_achilles_dict['N_rec']\n",
    "n_timepoints       = neuron_spike_times.shape[1]\n",
    "\n",
    "# print the shapes of the data:\n",
    "print(f\"Neuron spike times shape: {neuron_spike_times.shape}\")\n",
    "\n",
    "\"\"\" \n",
    "The dataset consists of the following variables:\n",
    "- neuron_spike_times: A binary matrix of shape (N_rec, n_timepoints) where N_rec is the number of\n",
    "    recorded neurons and n_timepoints is the number of timepoints in the recording. A value of 1 indicates\n",
    "    a spike from a neuron at a given timepoint.\n",
    "- position_readout: A vector of shape (n_timepoints,) representing the position of the mouse at each timepoint.\n",
    "- left_direction: A binary vector of shape (n_timepoints,) indicating whether the mouse is moving in the left direction.\n",
    "- right_direction: A binary vector of shape (n_timepoints,) indicating whether the mouse is moving in the right direction.\n",
    "- N_rec: The number of recorded neurons.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the neurons and timepoints for the scatter plot:\n",
    "neurons, timepoints = np.where(neuron_spike_times)\n",
    "\n",
    "# spike raster plot and histogram of spiking rate:\n",
    "fig = plt.figure(figsize=(6, 8))\n",
    "gs = gridspec.GridSpec(6, 1)\n",
    "\n",
    "# create the first subplot (3/4 of the figure)\n",
    "ax1 = plt.subplot(gs[0:4, :])\n",
    "ax1.scatter(timepoints, neurons, s=8.0, color='mediumaquamarine', alpha=1.0)\n",
    "#ax1.imshow(neuron_spike_times, aspect='auto', cmap='viridis', interpolation='none')\n",
    "plt.title(f\"1st {n_timepoints} timepoints of the hippocampus\\ndataset 'achilles' from CEBRA\")\n",
    "#ax1.set_xlabel(\"time [ms]\")\n",
    "ax1.set_xticks([])\n",
    "ax1.set_ylabel(\"neuron ID\")\n",
    "\n",
    "# create the second subplot:\n",
    "ax2 = plt.subplot(gs[4, :])\n",
    "hist_binwidth = 5.0\n",
    "t_bins = np.arange(np.amin(timepoints), np.amax(timepoints), hist_binwidth)\n",
    "n, bins = np.histogram(timepoints, bins=t_bins)\n",
    "heights = 1000 * n / (hist_binwidth * (N_rec))\n",
    "ax2.bar(t_bins[:-1], heights, width=hist_binwidth, color='violet')\n",
    "ax2.set_ylabel(\"firing rate\\n[Hz]\")\n",
    "ax2.set_xticks([])\n",
    "\n",
    "# create the third subplot:\n",
    "ax3 = plt.subplot(gs[5, :])\n",
    "#ax3.plot(np.arange(1000), position_readout, c = 'k')\n",
    "ax3.scatter(np.arange(n_timepoints)[left_direction == 1],  position_readout[left_direction == 1], \n",
    "         c='lightseagreen', label='left direction', s=1)\n",
    "ax3.scatter(np.arange(n_timepoints)[right_direction == 1], position_readout[right_direction == 1], \n",
    "         c='salmon', label='right direction', s=1)\n",
    "ax3.set_ylabel(f'position\\n[m]')\n",
    "ax3.set_xlabel(\"time [s]\")\n",
    "ax3.legend(loc=\"upper right\", frameon=True)\n",
    "ax3.set_xticks(np.arange(0, n_timepoints+1, 200))\n",
    "ax3.set_xticklabels(np.arange(0, 0.025*n_timepoints+1, 0.025*200).astype(\"int\")) \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(RESULTSPATH, 'AE_behavior_hippocampus_achilles_spike_raster.png'), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need again to rearrange the data. We need to **feed both neural spike times and position readout** into the VAE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataset by concatenating neuron spikes and position:\n",
    "neuron_spike_times_flat = neuron_spike_times.T  # shape: (timepoints, neurons)\n",
    "position_readout_flat = position_readout.reshape(-1, 1)  # shape: (timepoints, 1)\n",
    "combined_data = np.hstack((neuron_spike_times_flat, position_readout_flat))\n",
    "\n",
    "# convert the combined data to a tensor:\n",
    "combined_data_tensor = torch.tensor(combined_data).float()\n",
    "\n",
    "print(f\"Combined data shape: {combined_data.shape}\")  # should be (timepoints, neurons + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again define a custom dataset class for PyTorch's DataLoader. The `CustomDataset` class needs to be modified to include both the neural spike times and the behavior data as inputs for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        instance = self.data[idx, :]\n",
    "        return instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate the `CustomDataset` class and split the data into training and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset_tensor = CustomDataset(combined_data_tensor)\n",
    "train_size = int(0.9 * len(combined_dataset_tensor))\n",
    "test_size = len(combined_dataset_tensor) - train_size\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(combined_dataset_tensor, [train_size, test_size])\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=150, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=150, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📝 Implement the Variational Autoencoder (VAE) architecture\n",
    "Now, we define the VAE model. The VAE model consists of two parts: the encoder and the decoder. The encoder takes the input data and maps it to a latent space. The decoder takes the latent space representation and maps it back to the original input space. In our case, the encoder has two fully connected layers, and outputs the mean and the log-variance of the latent space representation. The decoder mirrors the encoder, and outputs the reconstructed input data.\n",
    "\n",
    "1. Implement the VAE architecture by completing the code below. Consider the following layers in the encoder and decoder:\n",
    "    - Encoder:\n",
    "      - Fully connected layer with 64 units and ReLU activation (in `encode()`), receiving the input data size\n",
    "      - Fully connected layer with 32 units and ReLU activation(in `encode()`),\n",
    "      - Fully connected layer with latent_size units for the mean,\n",
    "      - Fully connected layer with latent_size units for the log-variance.\n",
    "    - Decoder:\n",
    "      - should mirror the encoder architecture, with the last layer (in `decode()``) having a sigmoid activation function.\n",
    "2. Complete the `forward()` method to define the forward pass of the VAE.\n",
    "3. Implement the `reparameterize()` method to sample from the latent space using the reparameterization trick.\n",
    "\n",
    "Hint: You can mainly follow our example from the lecture, but make sure to adapt the architecture to the input size of the hippocampus dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here:\n",
    "# define the VAE architecture:\n",
    "#class VariationalAutoencoder(nn.Module):\n",
    "#    def __init__(self, input_size, latent_size):\n",
    "#        super(VariationalAutoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        # ...\n",
    "        \n",
    "        # Decoder\n",
    "        # ...\n",
    "\n",
    "#    def encode(self, x):\n",
    "        # ...\n",
    "#        return mu, logvar\n",
    "\n",
    "#    def reparameterize(self, mu, logvar):\n",
    "        # ...\n",
    "#        return mu + eps * std\n",
    "\n",
    "#    def decode(self, z):\n",
    "        # ...\n",
    "#        return torch.sigmoid(self.decoder_fc3(h4))\n",
    "\n",
    "#    def forward(self, x):\n",
    "        # ...\n",
    "#        return recon_x, mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📝 Define the loss function\n",
    "Next, we need to define the loss function. The loss function for the VAE consists of two parts: the reconstruction loss and the KL divergence loss. The reconstruction loss measures how well the model can reconstruct the input data. The KL divergence loss measures how well the latent space representation follows a normal distribution. The total loss is the sum of the reconstruction loss and the KL divergence loss. Here, we use the binary cross-entropy loss for the reconstruction loss since our data is binary.\n",
    "\n",
    "1. Implement the `loss_function()` method to define the loss function of the VAE. The method should return the total loss, the reconstruction loss, and the KL divergence loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here:\n",
    "#def vae_loss_function(recon_x, x, mu, logvar):\n",
    "    #...\n",
    "#    return BCE + KLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train our model. We define an according training loop and keep track of both the training and validation losses. Note, that the training can take a while:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility:\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# initialize model, optimizer, and device\n",
    "input_size = combined_data.shape[1]\n",
    "latent_size = 3\n",
    "model = VariationalAutoencoder(input_size=input_size, latent_size=latent_size)\n",
    "device = torch.device('mps') if torch.backends.mps.is_available() else torch.device('cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# training loop:\n",
    "epochs = 50\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        recon_batch, mu, logvar = model(batch)\n",
    "        loss = vae_loss_function(recon_batch, batch, mu, logvar)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # validation phase:\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = batch.to(device)\n",
    "            recon_batch, mu, logvar = model(batch)\n",
    "            val_loss += vae_loss_function(recon_batch, batch, mu, logvar).item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(test_loader.dataset)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_val_loss:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the training and validation losses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training and validation loss curves:\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('VAE Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VAE seems to have converged. We can now use the trained model to encode the data and visualize the latent space representation. We encode the data and plot the latent space representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move the model to evaluation mode and loop through the dataset to get the \n",
    "# latent space WITHOUT gradients (=no backpropagation; we don't want to update our model!):\n",
    "model.eval()\n",
    "latent_space = []\n",
    "with torch.no_grad():\n",
    "    for batch in DataLoader(dataset=combined_dataset_tensor, batch_size=150, shuffle=False):\n",
    "        batch = batch.to(device)\n",
    "        mu, logvar = model.encode(batch)\n",
    "        z = model.reparameterize(mu, logvar)\n",
    "        latent_space.append(z.cpu().numpy())\n",
    "\n",
    "# Concatenate latent space results\n",
    "latent_space = np.concatenate(latent_space, axis=0)\n",
    "\n",
    "# plot the 3D latent space:\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(latent_space[:, 0], latent_space[:, 1], latent_space[:, 2], c=np.arange(len(latent_space)), cmap='viridis')\n",
    "ax.set_title(\"3D Latent Space of VAE\")\n",
    "ax.set_xlabel('Latent Dimension 1')\n",
    "ax.set_ylabel('Latent Dimension 2')\n",
    "ax.set_zlabel('Latent Dimension 3')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📝 Replot the 3D latent space, color-coded by the position of the mouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code goes here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems, that our VAE model is able to structure the data in accordance with the mouse's position (kind of). One could further investigate the latent space representation and try to improve the model by tuning the hyperparameters or the architecture. However, we will stop here and move on to the part, where we train a new VAE with the goal, to predict the mouse's position from the neural data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\beta$-VAE and behavior prediction\n",
    "Let's try whether we can predict the position of the mouse from the latent space representation by inputting the neural spike times into the decoder. To achieve this, we need to modify the VAE model to have a secondary output that specifically predicts the behavior vector. This model will be trained in a way that it learns to both reconstruct the neural activity and predict the behavior vector from the latent space representation. After training, we can use only the neural data as input to the VAE and obtain the behavior prediction as output.\n",
    "\n",
    "To proceed, we need to adjust\n",
    "\n",
    "* data standardization to both the neural data and the behavior data,\n",
    "* additional data augmentation to the training set in order to increase the model's generalization capabilities, and\n",
    "* update the CustomDataset class to also include the behavior data as target output for the model and the data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the neural data and behavior data:\n",
    "scaler_neural = StandardScaler()\n",
    "scaler_behavior = StandardScaler()\n",
    "\n",
    "neural_data_scaled = scaler_neural.fit_transform(neuron_spike_times_flat)\n",
    "behavior_data_scaled = scaler_behavior.fit_transform(position_readout_flat)\n",
    "\n",
    "# convert data to tensors:\n",
    "neural_data_tensor = torch.tensor(neural_data_scaled).float()\n",
    "behavior_data_tensor = torch.tensor(behavior_data_scaled).float()\n",
    "\n",
    "# define a data augmentation function:\n",
    "def augment_data(neural_data, behavior_data, noise_level=0.01):\n",
    "    noise = torch.randn_like(neural_data) * noise_level\n",
    "    augmented_neural_data = neural_data + noise\n",
    "    return augmented_neural_data, behavior_data\n",
    "\n",
    "# custom dataset with augmentation:\n",
    "class AugmentedDataset(Dataset):\n",
    "    def __init__(self, neural_data, behavior_data, augment=False):\n",
    "        self.neural_data = neural_data\n",
    "        self.behavior_data = behavior_data\n",
    "        self.augment = augment\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.neural_data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        neural_activity = self.neural_data[idx, :]\n",
    "        behavior = self.behavior_data[idx]\n",
    "        if self.augment:\n",
    "            neural_activity, behavior = augment_data(neural_activity, behavior)\n",
    "        return {\"neural_activity\": neural_activity, \"behavior\": behavior}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate dataset and split into train and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset_tensor = AugmentedDataset(neural_data_tensor, behavior_data_tensor, augment=True)\n",
    "train_size = int(0.7 * len(combined_dataset_tensor))\n",
    "test_size = len(combined_dataset_tensor) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(combined_dataset_tensor, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the VAE model with a secondary output for the behavior prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# define the VAE with behavior prediction\n",
    "class VAEWithBehaviorPrediction(nn.Module):\n",
    "    def __init__(self, input_size, latent_size, behavior_size):\n",
    "        super(VAEWithBehaviorPrediction, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder_fc1 = nn.Linear(input_size, 64)\n",
    "        self.encoder_fc2 = nn.Linear(64, 32)\n",
    "        self.encoder_fc3 = nn.Linear(32, 16)\n",
    "        self.encoder_fc4 = nn.Linear(16, 8)  # Added layer\n",
    "        self.fc_mu = nn.Linear(8, latent_size)\n",
    "        self.fc_logvar = nn.Linear(8, latent_size)\n",
    "        \n",
    "        # Decoder for neural data reconstruction\n",
    "        self.decoder_fc1 = nn.Linear(latent_size, 8)  # Adjusted input size\n",
    "        self.decoder_fc2 = nn.Linear(8, 16)\n",
    "        self.decoder_fc3 = nn.Linear(16, 32)\n",
    "        self.decoder_fc4 = nn.Linear(32, 64)\n",
    "        self.decoder_fc5 = nn.Linear(64, input_size)  # Added layer\n",
    "        \n",
    "        # Behavior prediction from latent space\n",
    "        dropout_rate = 0.3\n",
    "        self.behavior_fc1 = nn.Linear(latent_size, 32)\n",
    "        self.dropout1 = nn.Dropout(p=dropout_rate)\n",
    "        self.behavior_fc2 = nn.Linear(32, 16)\n",
    "        self.dropout2 = nn.Dropout(p=dropout_rate)\n",
    "        self.behavior_fc3 = nn.Linear(16, 8)\n",
    "        self.dropout3 = nn.Dropout(p=dropout_rate)\n",
    "        self.behavior_fc4 = nn.Linear(8, behavior_size)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = torch.relu(self.encoder_fc1(x))\n",
    "        h2 = torch.relu(self.encoder_fc2(h1))\n",
    "        h3 = torch.relu(self.encoder_fc3(h2))\n",
    "        h4 = torch.relu(self.encoder_fc4(h3))  # Added layer\n",
    "        mu = self.fc_mu(h4)\n",
    "        logvar = self.fc_logvar(h4)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h4 = torch.relu(self.decoder_fc1(z))\n",
    "        h5 = torch.relu(self.decoder_fc2(h4))\n",
    "        h6 = torch.relu(self.decoder_fc3(h5))\n",
    "        h7 = torch.relu(self.decoder_fc4(h6))  # Added layer\n",
    "        return torch.sigmoid(self.decoder_fc5(h7))\n",
    "\n",
    "    def predict_behavior(self, z):\n",
    "        h7 = torch.relu(self.behavior_fc1(z))\n",
    "        h8 = torch.relu(self.behavior_fc2(h7))\n",
    "        h9 = torch.relu(self.behavior_fc3(h8))\n",
    "        return self.behavior_fc4(h9)  # no activation, as this is a regression task\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon_x = self.decode(z)\n",
    "        pred_behavior = self.predict_behavior(z)\n",
    "        return recon_x, pred_behavior, mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and adjust the loss function accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE loss function with behavior prediction:\n",
    "# you will adjust the function later (see exercise below)\n",
    "def vae_behavior_loss(recon_x, x, pred_behavior, behavior, mu, logvar, beta=0.1):\n",
    "    # Reconstruction loss for neural activity\n",
    "    recon_loss = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
    "    \n",
    "    # Kullback-Leibler divergence for VAE, scaled by beta\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    # Behavior prediction loss (mean squared error)\n",
    "    behavior_loss = nn.functional.mse_loss(pred_behavior, behavior, reduction='sum')\n",
    "    \n",
    "    # Total loss (balance reconstruction, KLD, and behavior prediction)\n",
    "    return recon_loss +  KLD + behavior_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the model and optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility:\n",
    "torch.manual_seed(2) # 5! 13 32 43!\n",
    "\n",
    "input_size = neuron_spike_times_flat.shape[1]  # only neural data size\n",
    "latent_size = 3  # size of latent space\n",
    "behavior_size = 8  # size of behavior vector (position)\n",
    "\"\"\" \n",
    "Actually, we should set behavior_size to 1, as we only have one behavior variable (position). However, the predictor \n",
    "performs better for a higher \"behavior latent space\" size.\n",
    "\"\"\"\n",
    "model = VAEWithBehaviorPrediction(input_size=input_size, latent_size=latent_size, behavior_size=behavior_size)\n",
    "device = torch.device('mps') if torch.backends.mps.is_available() else torch.device('cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# adjust the learning rate and optimizer later (see exercise below):\n",
    "learning_rate = 0.01\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to make some adjustments to the training loop to include the behavior prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop:\n",
    "kl_beta = 0.5  # 0.5 best value\n",
    "epochs = 50\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_behavior_losses = []\n",
    "val_behavior_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_behavior_loss = 0\n",
    "    for batch in train_loader:\n",
    "        neural_activity = batch['neural_activity'].to(device)\n",
    "        behavior = batch['behavior'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        recon_neural, pred_behavior, mu, logvar = model(neural_activity)\n",
    "        \n",
    "        # Compute losses\n",
    "        loss = vae_behavior_loss(recon_neural, neural_activity, pred_behavior, behavior, mu, logvar, kl_beta)\n",
    "        behavior_loss = nn.functional.mse_loss(pred_behavior, behavior, reduction='sum')\n",
    "        \n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        train_behavior_loss += behavior_loss.item()\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "    avg_train_behavior_loss = train_behavior_loss / len(train_loader.dataset)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_behavior_losses.append(avg_train_behavior_loss)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_behavior_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            neural_activity = batch['neural_activity'].to(device)\n",
    "            behavior = batch['behavior'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            recon_neural, pred_behavior, mu, logvar = model(neural_activity)\n",
    "            \n",
    "            # Compute validation losses\n",
    "            val_loss += vae_behavior_loss(recon_neural, neural_activity, pred_behavior, behavior, mu, logvar, kl_beta).item()\n",
    "            val_behavior_loss += nn.functional.mse_loss(pred_behavior, behavior, reduction='sum').item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(test_loader.dataset)\n",
    "    avg_val_behavior_loss = val_behavior_loss / len(test_loader.dataset)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_behavior_losses.append(avg_val_behavior_loss)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_val_loss:.6f}, '\n",
    "          f'Train Behavior Loss: {avg_train_behavior_loss:.6f}, Val Behavior Loss: {avg_val_behavior_loss:.6f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at both the loss curves of the neural activity reconstruction and the behavior prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training and validation loss curves:\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# plot overall training and validation loss:\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Overall VAE Training and Validation Loss')\n",
    "\n",
    "# plot behavior prediction loss specifically\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_behavior_losses, label='Train Behavior Loss')\n",
    "plt.plot(val_behavior_losses, label='Validation Behavior Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Behavior Prediction Loss')\n",
    "plt.legend()\n",
    "plt.title('Behavior Prediction Loss in Training and Validation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(RESULTSPATH, 'VAE_behavior_prediction_loss_detailed.png'), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like our updated VAE does not perform well on the behavior prediction task. This could be due to the fact that the latent space representation does not contain enough information to predict the behavior. However, let's take a look at the actual prediction performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction of behavior from neural activity:\n",
    "model.eval()\n",
    "neural_data_only_loader = DataLoader(dataset=combined_dataset_tensor, batch_size=128, shuffle=False)\n",
    "predicted_behaviors = []\n",
    "actual_behaviors = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in neural_data_only_loader:\n",
    "        neural_activity = batch['neural_activity'].to(device)\n",
    "        behavior = batch['behavior'].to(device)\n",
    "        \n",
    "        _, pred_behavior, _, _ = model(neural_activity)\n",
    "        predicted_behaviors.append(pred_behavior.cpu().numpy())\n",
    "        actual_behaviors.append(behavior.cpu().numpy())\n",
    "\n",
    "# Concatenate and plot the behavior prediction\n",
    "predicted_behaviors = np.concatenate(predicted_behaviors, axis=0)\n",
    "actual_behaviors = np.concatenate(actual_behaviors, axis=0)\n",
    "\n",
    "if behavior_size >1:\n",
    "    predicted_behaviors_avrg = np.mean(predicted_behaviors, axis=1)\n",
    "    \"\"\"\n",
    "    due to the higher dimension of the behavior latent space, we need to average the \n",
    "    predictions to get a 1D behavior prediction array for plotting.\n",
    "    \"\"\"\n",
    "else:\n",
    "    predicted_behaviors_avrg = predicted_behaviors\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(actual_behaviors, label='Actual Behavior', color='blue')\n",
    "plt.plot(predicted_behaviors_avrg, label='Predicted Behavior', color='orange', alpha=0.7)\n",
    "plt.xlabel('Time Point')\n",
    "plt.ylabel('Behavior (Position)')\n",
    "plt.legend()\n",
    "plt.title('Actual vs Predicted Behavior')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(RESULTSPATH, 'VAE_behavior_prediction.png'), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, the model seems to perform quite well on the behavior prediction task. This could be due to the fact that the behavior is highly correlated with the neural activity. However, the model might not generalize well to new data. \n",
    "\n",
    "As a last step, let's visualize the latent space representation and the behavior prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract latent space representation from the full dataset:\n",
    "model.eval()\n",
    "latent_mu_values = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in neural_data_only_loader:\n",
    "        neural_activity = batch['neural_activity'].to(device)\n",
    "        mu, _ = model.encode(neural_activity)\n",
    "        latent_mu_values.append(mu.cpu().numpy())\n",
    "\n",
    "# concatenate the latent space representations:\n",
    "latent_mu_values = np.concatenate(latent_mu_values, axis=0)\n",
    "\n",
    "# plotting the 3D latent space:\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "sc = ax.scatter(latent_mu_values[:, 0], latent_mu_values[:, 1], latent_mu_values[:, 2], \n",
    "                c=np.arange(len(latent_mu_values)), cmap='plasma', s=5, alpha=0.7)\n",
    "plt.colorbar(sc, label='Time point')\n",
    "ax.set_title(\"3D latent space of VAE\")\n",
    "ax.set_xlabel(\"Latent Dimension 1\")\n",
    "ax.set_ylabel(\"Latent Dimension 2\")\n",
    "ax.set_zlabel(\"Latent Dimension 3\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(RESULTSPATH, 'VAE_3D_latent_space.png'), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📝 Replot latent space with real behavior color-coding\n",
    "1. Replot the latent space with real behavior color-coding (left/right direction)\n",
    "2. Add 3x 2D scatter plots for the latent space representation, color-coded by the left/right direction of the mouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📝 Tune the model\n",
    "The visualization of the 3D VAE latent space suggests some unusual structure. The data points are almost aligned along a thin line, with very little spread across the latent dimensions. This pattern could indicate several possible issues or limitations in how the VAE is currently representing the data:\n",
    "\n",
    "* **Insufficient capacity of the latent space**: The VAE's latent space may be too limited (or regularized too heavily) to capture the complexity of the input data, resulting in a \"collapsed\" latent space where most points lie along a single dimension or line. This could be due to a very small latent space (e.g., only 3 dimensions) for highly complex data.\n",
    "* **Too strong regularization from the KL divergence term**: The Kullback-Leibler divergence (KL term) in the VAE loss might be overpowering the reconstruction loss, pushing the latent variables closer to a single point or a narrow region in the latent space. This happens if the balance between reconstruction loss and KL divergence isn't optimized correctly. \n",
    "* **Inadequate encoder network complexity**: The encoder network might be too simple to map the input data to a well-distributed latent space. Increasing the complexity of the encoder by adding more layers or increasing the layer sizes could improve the diversity in the latent space.\n",
    "* Data characteristics: It's also possible that the data itself doesn’t have sufficient variation for a 3D latent space to learn distinct patterns. This is less likely but can happen if the data doesn't contain much underlying diversity.\n",
    "* **High-dimensional data compression**: The data might be too high-dimensional for the VAE to compress effectively into just 3 latent dimensions, causing a form of \"information bottleneck\". Increasing the latent dimensionality (e.g., from 3 to 5 or 10) could help in better representing the data.\n",
    "\n",
    "Let's try to improve the model implementing some improvements. We will address some of the issues mentioned above:\n",
    "\n",
    "Process the suggestions below step-by-step, .i.e., do not implement all suggestions at once and retrain the model after each step. You can also try to implement your own ideas to improve the model.\n",
    "\n",
    "1. Change the loss function to a **$beta$ regulated loss function**.\n",
    "2. Add **weight decay** (`weight_decay=0.001`) to the optimizer to reduce overfitting. Fine-tune the model by iteratively adjusting the **learning rate** and the weight decay. Try smaller learning rates and weight decays. You will most likel not get a perfect convergence of the model, but try to get a better performance on the behavior prediction task and especially in the latent space structure.\n",
    "3. When you're satisfied with the model's performance, change the **random seed** and retrain the model to see if the model's performance is consistent across different random seeds. What do you notice?\n",
    "\n",
    "SOL 3: Your answer goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dimredcution",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
