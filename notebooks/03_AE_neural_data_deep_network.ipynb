{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands on session: Autoencoders (1)\n",
    "This is the first of two examples, where we will use Autoencoders to reduce the dimensionality of the calcium imaging data. We will use the same dataset as in the previous exercises (PCA, Clustering methods). \n",
    "\n",
    "In this example, we encode the calcium data for each neuron, i.e., we represent each neuron in a lower-dimensional space, compressing the time information. \n",
    "\n",
    "This exercise refers to Chapter 4 \"Autoencoders\" of the \"Dimensionality reduction in neuroscience\" course (tutor: Fabrizio Musacchio, Oct 17, 2024)\n",
    "\n",
    "## Acknowledgements:\n",
    "The dataset is from the 2023's course 'data analysis techniques in neuroscience' by the Chen Institute for Neuroscience at Caltech:  \n",
    "\n",
    "<https://github.com/cheninstitutecaltech/Caltech_DATASAI_Neuroscience_23>\n",
    "\n",
    "and originally from the paper:\n",
    "\n",
    "Remedios, R., Kennedy, A., Zelikowsky, M. et al. Social behaviour shapes hypothalamic neural  ensemble representations of conspecific sex. Nature 550, 388‚Äì392 (2017). <https://doi.org/10.1038/nature23885>\n",
    "\n",
    "## Dataset\n",
    "We will work with the same calcium imaging data from the previous exercise (PCA). For details, please refer to the previous exercise.\n",
    "\n",
    "## Environment setup\n",
    "For reproducibility:\n",
    "\n",
    "```bash\n",
    "conda create -n dimredcution python=3.11 mamba -y\n",
    "conda activate dimredcution\n",
    "mamba install ipykernel matplotlib numpy scipy scikit-learn -y\n",
    "mamba install pytorch torchvision -c pytorch\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start, as usual, by importing the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% IMPORTS\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "\n",
    "from mpl_toolkits import mplot3d\n",
    "from scipy.io import loadmat\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# for reproducibility, we set the random seed:\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the torch version and that the GPU is available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify torch version and GPU availability:\n",
    "print(f\"torch backend MPS is available? {torch.backends.mps.is_available()}\")\n",
    "print(f\"current PyTorch installation built with MPS activated? {torch.backends.mps.is_built()}\")\n",
    "print(f\"check the torch MPS backend: {torch.device('mps')}\")\n",
    "print(f\"test torch tensor on MPS: {torch.tensor([1,2,3], device='mps')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On macOS, you need to move your later model to the MPS device, if you want to use the Mac's GPU (Apple Silicon):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('mps')\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otherwise, change this line to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the path to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/'\n",
    "DATA_FILENAME = 'hypothalamus_calcium_imaging_remedios_et_al.mat'\n",
    "DATA_FILE = os.path.join(DATA_PATH, DATA_FILENAME)\n",
    "\n",
    "RESULTSPATH = '../results/'\n",
    "# check whether the results path exists, if not, create it:\n",
    "if not os.path.exists(RESULTSPATH):\n",
    "    os.makedirs(RESULTSPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothalamus_data = loadmat(DATA_FILE)\n",
    "\n",
    "# extract the N main data arrays into N separate variables:\n",
    "neural_data   = hypothalamus_data['neural_data']\n",
    "attack_vector = hypothalamus_data['attack_vector']\n",
    "gender_vector = hypothalamus_data['sex_vector']\n",
    "\n",
    "# we just take a short snippet of the data to speed up the computations:\n",
    "N_datapoints = 10000\n",
    "neural_data_short = neural_data[:, 0:N_datapoints]\n",
    "print(f\"The shape of the data is: {neural_data_short.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a custom dataset class for the neural data (this is necessary for Pytorch's DataLoader):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        instance = self.data[idx, :]\n",
    "        sample = {\"data\": instance}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch works best with tensors, so we convert the numpy array to a tensor. A tensor is a container that stores data in $N$ dimensions. A matrix is a special case of a tensor that is 2D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_data_float_tensor = torch.tensor(neural_data_short).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Inspect the shape of the `neural_data_float_tensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply the `CustomDataset` class to our neural data tensor `neural_data_float_tensor`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_dataset_tensor = CustomDataset(neural_data_float_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Inspect `neural_dataset_tensor` created by the `CustomDataset` class\n",
    "1. Inspect the `.__len__()` method.\n",
    "2. Inspect the `.__getitem__(0)` method.\n",
    "3. Inspect the `.__getitem__(0).keys()` method.\n",
    "4. Inspect the `.__getitem__(0)['data'].shape` method.\n",
    "5. Also inspect the above methods for another index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to break our data into a train (90% of the data) and test (10% of the data) split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(np.floor(0.9 * len(neural_dataset_tensor)))\n",
    "test_size = len(neural_dataset_tensor) - train_size\n",
    "\n",
    "print(train_size, test_size)\n",
    "# train_size, test_size is (103, 12), ie., 90% (=103/115) and 10% (=12/115) neurons of the full data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now using the `torch.utils.data.random_split()` function we split the data into a  train set and a test set; for this, we could use this command:\n",
    "\n",
    "```python\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(neural_dataset_tensor, [train_size, test_size])\n",
    "```\n",
    "\n",
    "However, we want to keep track of the original neuron IDs, so we will do it manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(neural_dataset_tensor)\n",
    "indices = list(range(dataset_size))\n",
    "#train_dataset, test_dataset = torch.utils.data.random_split(neural_dataset_tensor, [train_size, test_size])\n",
    "\n",
    "# randomly split the indices:\n",
    "train_indices, test_indices = torch.utils.data.random_split(indices, [train_size, test_size])\n",
    "\n",
    "# create subsets using the indices:\n",
    "train_dataset = Subset(neural_dataset_tensor, train_indices)\n",
    "test_dataset  = Subset(neural_dataset_tensor, test_indices)\n",
    "\n",
    "# To re-identify the original ID of each neuron in the train_dataset, we store the original indices:\n",
    "original_train_ids = train_indices.indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Create the data loaders\n",
    "Using the `torch.utils.data.DataLoader()`, make a train data loader and a test data loader. Make sure to use the dataset, batch_size, and shuffle parameters when you call the function. If you are not familiar with the DataLoader, please check the Pytorch documentation. For simplicity, set the batch size to be larger than all the data you have. This is okay for our small dataset, but in practice, you would want to set the batch size to be a smaller number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here:\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(...)\n",
    "# test_loader  = torch.utils.data.DataLoader(...)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Now, define the Autoencoder model\n",
    "1. Create a class `Autoencoder` that inherits from `torch.nn.Module`. Create a linear `encoder` with the following layers:\n",
    "   - A linear layer with 10000 input features and 5000 output features.\n",
    "   - A ReLU activation function.\n",
    "   - A linear layer with 5000 input features and 2500 output features.\n",
    "   - A ReLU activation function.\n",
    "   - A linear layer with 2500 input features and 625 output features.\n",
    "   - A ReLU activation function.\n",
    "   - A linear layer with 625 input features and 156 output features.\n",
    "   - A ReLU activation function.\n",
    "   - A linear layer with 156 input features and 39 output features.\n",
    "   - A ReLU activation function.\n",
    "   - A linear layer with 39 input features and 3 output features.\n",
    "2. Create an according linear `decoder`.\n",
    "3. Define a `forward` method that takes an input x and returns the embedding and the reconstruction of the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here:\n",
    "# class Autoencoder(...):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#        \n",
    "#         self.encoder = torch.nn.Sequential(\n",
    "#             ...\n",
    "#         )\n",
    "          \n",
    "#         self.decoder = torch.nn.Sequential(\n",
    "#             ...\n",
    "#         )\n",
    "  \n",
    "#     def forward(self, x):\n",
    "#         ...\n",
    "#         return embedding, reconstruction\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù  Train the autoencoder\n",
    "\n",
    "Prepare and create a training loop to train the autoencoder on the training data:\n",
    "1. Train the autoencoder for 250 epochs.\n",
    "2. Keep track of the training and validation losses.\n",
    "3. As loss function, use the Mean Squared Error (MSE) loss (`torch.nn.MSELoss()`).\n",
    "4. Use the Adam optimizer (`torch.optim.Adam(...)`) with an initial learning rate of 1e-2, a `weight_decay` of 1e-8 and a `step_size` of 1.\n",
    "5. Follow further instructions below to complete the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here:\n",
    "\n",
    "# set training parameters:\n",
    "# epochs =\n",
    "\n",
    "# prepare lists to store the outputs and losses:\n",
    "\n",
    "# model initialization (just uncomment the line below):\n",
    "# model = Autoencoder()\n",
    "\n",
    "# on macOS, move the model to the MPS device:\n",
    "# device = torch.device('mps')\n",
    "# model = model.to(device)\n",
    "\n",
    "# otherwise, move the model to the GPU if available:\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = model.to(device)\n",
    "\n",
    "\n",
    "# using Mean-Squared-Error MSE Loss function:\n",
    "# loss_function = ...\n",
    "\n",
    "# using an Adam Optimizer:\n",
    "# learning_rate = ...\n",
    "# optimizer = torch.optim.Adam(...)\n",
    "\n",
    "# learning rate scheduler (just uncomment the line below):\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=1, gamma=0.98)\n",
    "\n",
    "\n",
    "# define the training loop:\n",
    "#for epoch in range(epochs):\n",
    "#    # training on train set:\n",
    "#    model.train()\n",
    "#    \n",
    "#    # loop through your training data;\n",
    "#    for batch_idx, batch in enumerate(train_loader):\n",
    "\n",
    "        # STEP 1: pull out the data from your batch\n",
    "        # batch_data = ...\n",
    "        \n",
    "        # STEP 2: get the reconstructed data from the Autoencoder Output\n",
    "        # ...\n",
    "\n",
    "        # STEP 3: calculate the loss function between the reconstruction and original data\n",
    "        # loss = ...\n",
    "\n",
    "        # set gradients to zero (just uncomment the line below):\n",
    "        #optimizer.zero_grad()\n",
    "        \n",
    "        # the gradient is computed and stored (just uncomment the line below):\n",
    "        #loss.backward()\n",
    "        \n",
    "        # perform the parameter update (just uncomment the line below):\n",
    "        #optimizer.step()\n",
    "\n",
    "        # storing the losses in a list for plotting\n",
    "        # ...\n",
    "       \n",
    "    # put model into evaluation mode (just uncomment the line below):\n",
    "    # model.eval()\n",
    "    \n",
    "    # loop through your testing/validation data:\n",
    "    #for validation_batch_idx, validation_batch in enumerate(test_loader):\n",
    "    \n",
    "        # STEP_4: pull out the data from your validation batch\n",
    "        # validation_batch_data = ...\n",
    "        \n",
    "        # STEP 5: get the reconstructed data from the Autoencoder Output\n",
    "        # ...\n",
    "        \n",
    "        # STEP 6: calculate the loss function between the reconstrucion and original data\n",
    "        # val_loss = ...\n",
    "        \n",
    "        # STEP 7: append the validation losses to the validation loss list\n",
    "        #... \n",
    "    \n",
    "    # STEP 8: append the outputs to both the validation and train outputs lists in the form of:\n",
    "    # (epochs, original data, reconstruction, embedding). Don't forget to transform your tensors into numpy arrays!!!\n",
    "    # validation_outputs.append(...)\n",
    "    # outputs.append(...)\n",
    "    \n",
    "    # uncomment the line below to print the epoch number and the losses:\n",
    "    # print(f'Epoch {epoch+1}/{epochs}, Train Loss: {losses[-1]:.6f}, Val Loss: {validation_losses[-1]:.6f}')\n",
    "\n",
    "#print(len(outputs))\n",
    "\n",
    "#print(f\"epoch number: {outputs[-1][0]}\") # epoch number\n",
    "#print(f\"original data: {outputs[-1][-1].shape}\") # original data\n",
    "#print(f\"reconstructed data{outputs[-1][-2].shape}\") # reconstructed data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Assess the loss curves\n",
    "1. Plot the training and validation loss curves (i.e., your previously stored losses).\n",
    "2. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you are unsatisfied with the results, you can try to improve the model by \n",
    "\n",
    "* changing the architecture: add Dropout layers (`torch.nn.Dropout()`) with a dropout rate of 0.5 to your encoder.\n",
    "* changing the learning rate; try 1e-3 or 1e-4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù For a sample neuron, plot the original and reconstructed data for the last epoch\n",
    "\n",
    "Hint: plot  \n",
    "outputs[epoch_idx][1][neuron_idx, :] and  \n",
    "outputs[epoch_idx][2][neuron_idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Compare all neurons: original vs. reconstructed\n",
    "1. Plot the original data and the reconstructed data with Matplotlib's `imshow`\n",
    "2. Plot the original validation data and the reconstructed validation data with Matplotlib's `imshow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Perform a PCA in the reconstructed data (2D comparison)\n",
    "We now want to take a look at the reconstructed on PCA latent space:\n",
    "1. Perform a PCA on the (original) train data and the reconstructed train data. Use three components.\n",
    "2. Plot the first two PCs of each PCA (PCA of the original data and the reconstructed data) in a single 2D plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Investigate the latent space of the AE (3D)\n",
    "\n",
    "1. Plot the AE embedding space in 3D at the first epoch (stored in `outputs[0][-1][:, 0]`, `outputs[0][-1][:, 1]`, `outputs[0][-1][:, 2]`)\n",
    "2. Plot the AE embedding space in 3D at the last epoch (stored in `outputs[-1][-1][:, 0]`, `outputs[-1][-1][:, 1]`, `outputs[-1][-1][:, 2]`). What do you notice by comparing the two plots/epochs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Replot the AE embedding scatter plot of the last epoch, now color-coded by the original data (`original_train_ids`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Cluster the AE embedded space of the last epoch using k-means (with 2 clusters) and replot the embedded space color-coded with the cluster labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Resort the neural data\n",
    "Let's check whether the clustering is consistent with the original data. To do this, sort the neural data according to the cluster labels and plot the sorted data. Compare with the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here:\n",
    "\n",
    "# resort the train_dataset array in such a way, that all neurons of cluster 0 are first, \n",
    "# and all neurons of cluster 1 are second:\n",
    "\n",
    "\n",
    "# now plot the sorted neural data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do you observe? What can you interpret from it?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dimredcution",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
